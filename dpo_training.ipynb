{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# On-device Direct Preference Optimization (DPO) \n",
        "\n",
        "This notebook can be used instead of the `02_src/train_dpo.py`.\n",
        "- Launch Jupyter from the repo root so paths resolve.\n",
        "- Set your Hugging Face token in `00_configs/secrets.toml` or `HF_TOKEN`.\n",
        "- Get a model access approvel that you would like to use.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70f3bba3",
      "metadata": {},
      "source": [
        "## Check the repository root"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c071f7a7",
      "metadata": {},
      "source": [
        "### You should assign the repository root for your device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "710a852b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Repo root: c:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\n",
            "Source dir: c:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\02_src\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "REPO_ROOT = Path.cwd()\n",
        "if not (REPO_ROOT / \"00_configs\").exists():\n",
        "    for parent in REPO_ROOT.parents:\n",
        "        if (parent / \"00_configs\").exists():\n",
        "            REPO_ROOT = parent\n",
        "            break\n",
        "\n",
        "if not (REPO_ROOT / \"00_configs\").exists():\n",
        "    raise RuntimeError(\"Could not find repo root containing 00_configs.\")\n",
        "\n",
        "SRC_DIR = REPO_ROOT / \"02_src\"\n",
        "if str(SRC_DIR) not in sys.path:\n",
        "    sys.path.insert(0, str(SRC_DIR))\n",
        "\n",
        "print(\"Repo root:\", REPO_ROOT)\n",
        "print(\"Source dir:\", SRC_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9f056ab",
      "metadata": {},
      "source": [
        "## Check the hardware, CUDA, and project modules"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa36a2c8",
      "metadata": {},
      "source": [
        "### Check the hardware condition and whether CUDA is available. You need enough memory to run this code (e.g.,mem_free/total_GB: 3.5/4.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "deab5335",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DPO readiness: NOT OK\n",
            " - VRAM 4.0 GB < recommended 26 GB.\n",
            " - Free VRAM 0.0 GB is low; close other apps.\n",
            " - 4-bit bitsandbytes can be unstable on native Windows; WSL recommended.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "import re\n",
        "from pathlib import Path\n",
        "import torch\n",
        "\n",
        "def parse_model_size_b(name: str) -> float | None:\n",
        "    m = re.search(r\"(\\d+(?:\\.\\d+)?)\\s*B\", name.replace(\"-\", \"\"))\n",
        "    return float(m.group(1)) if m else None\n",
        "\n",
        "cfg = json.loads(Path(\"00_configs/dpo.json\").read_text(encoding=\"utf-8\"))\n",
        "model_name = cfg.get(\"model_name\", \"\")\n",
        "load_in_4bit = cfg.get(\"load_in_4bit\", False)\n",
        "max_seq_length = cfg.get(\"max_seq_length\", 512)\n",
        "batch = cfg.get(\"per_device_train_batch_size\", 1)\n",
        "\n",
        "size_b = parse_model_size_b(model_name) or 1.0\n",
        "\n",
        "# Conservative VRAM heuristic for DPO (policy + ref model)\n",
        "if load_in_4bit:\n",
        "    if size_b <= 1.3:\n",
        "        rec = 8\n",
        "    elif size_b <= 3:\n",
        "        rec = 12\n",
        "    elif size_b <= 7:\n",
        "        rec = 24\n",
        "    else:\n",
        "        rec = 32\n",
        "else:\n",
        "    rec = 24 if size_b <= 1.3 else (32 if size_b <= 3 else 48)\n",
        "\n",
        "if max_seq_length > 512:\n",
        "    rec += 4\n",
        "if batch > 1:\n",
        "    rec += 4\n",
        "rec += 2  # DPO loads two models + overhead\n",
        "\n",
        "status = \"OK\"\n",
        "reasons = []\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    status = \"NOT OK\"\n",
        "    reasons.append(\"CUDA not available.\")\n",
        "else:\n",
        "    props = torch.cuda.get_device_properties(0)\n",
        "    total_gb = props.total_memory / (1024**3)\n",
        "    free_gb = torch.cuda.mem_get_info()[0] / (1024**3)\n",
        "\n",
        "    if total_gb < rec:\n",
        "        status = \"MARGINAL\" if total_gb >= rec - 2 else \"NOT OK\"\n",
        "        reasons.append(f\"VRAM {total_gb:.1f} GB < recommended {rec} GB.\")\n",
        "    if free_gb < rec * 0.8:\n",
        "        reasons.append(f\"Free VRAM {free_gb:.1f} GB is low; close other apps.\")\n",
        "\n",
        "if os.name == \"nt\" and load_in_4bit:\n",
        "    reasons.append(\"4-bit bitsandbytes can be unstable on native Windows; WSL recommended.\")\n",
        "\n",
        "print(f\"DPO readiness: {status}\")\n",
        "for r in reasons:\n",
        "    print(\" -\", r)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "9ff506fc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda_available: True\n",
            "device_name: NVIDIA RTX A1000 Laptop GPU\n",
            "mem_free/total_GB: 0.00/4.29\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(\"cuda_available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"device_name:\", torch.cuda.get_device_name(0))\n",
        "    free, total = torch.cuda.mem_get_info()\n",
        "    print(f\"mem_free/total_GB: {free/1e9:.2f}/{total/1e9:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de8557b8",
      "metadata": {},
      "source": [
        "### Project modules from the project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "16107dc9",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Imports OK\n"
          ]
        }
      ],
      "source": [
        "import importlib\n",
        "\n",
        "train_dpo = importlib.import_module(\"train_dpo\")\n",
        "run_inference = importlib.import_module(\"run_inference\")\n",
        "merge_lora = importlib.import_module(\"merge_lora\")\n",
        "data_utils = importlib.import_module(\"utils.data_utils\")\n",
        "formatting = importlib.import_module(\"utils.formatting\")\n",
        "\n",
        "# eval_module = importlib.import_module(\"eval.evaluate\")\n",
        "print(\"Imports OK\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ae7ca1f",
      "metadata": {},
      "source": [
        "### Confirm the configuration. You can adjust the paths and hyperparameters in 00_configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3fac907f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration loaded from c:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\00_configs\\dpo.json\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'model_name': 'meta-llama/Llama-3.2-1B-Instruct',\n",
              " 'dataset_hf': '01_data\\\\dpo\\\\train.jsonl',\n",
              " 'output_dir': '04_models\\\\adapters\\\\output_dpo',\n",
              " 'max_seq_length': 512,\n",
              " 'max_prompt_length': 256,\n",
              " 'max_target_length': 256,\n",
              " 'num_train_epochs': 3,\n",
              " 'per_device_train_batch_size': 1,\n",
              " 'gradient_accumulation_steps': 8,\n",
              " 'learning_rate': 2e-05,\n",
              " 'lr_scheduler_type': 'cosine',\n",
              " 'warmup_ratio': 0.02,\n",
              " 'weight_decay': 0.01,\n",
              " 'dataloader_num_workers': 2,\n",
              " 'logging_steps': 5,\n",
              " 'save_steps': 20,\n",
              " 'save_total_limit': 2,\n",
              " 'fp16': True,\n",
              " 'bf16': False,\n",
              " 'optim': 'paged_adamw_8bit',\n",
              " 'gradient_checkpointing': True,\n",
              " 'lora_r': 8,\n",
              " 'lora_alpha': 16,\n",
              " 'lora_dropout': 0.05,\n",
              " 'lora_target_modules': ['q_proj',\n",
              "  'k_proj',\n",
              "  'v_proj',\n",
              "  'o_proj',\n",
              "  'gate_proj',\n",
              "  'up_proj',\n",
              "  'down_proj'],\n",
              " 'load_in_4bit': True,\n",
              " 'bnb_4bit_use_double_quant': True,\n",
              " 'bnb_4bit_quant_type': 'nf4',\n",
              " 'bnb_4bit_compute_dtype': 'float16',\n",
              " 'dpo_beta': 0.1,\n",
              " 'seed': 42,\n",
              " 'dataset_split': 'train',\n",
              " 'max_train_samples': None,\n",
              " 'dataset_shuffle': True,\n",
              " 'num_proc': None}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Purpose: load the DPO training config from 00_configs/dpo.json.\n",
        "CONFIG_PATH = REPO_ROOT / \"00_configs\" / \"dpo.json\"\n",
        "config = train_dpo.load_config(CONFIG_PATH)\n",
        "config"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ae06b90",
      "metadata": {},
      "source": [
        "### HF token check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a298be98",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Secrets loaded from C:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\00_configs\\secrets.toml\n",
            "HuggingFace token configured\n",
            "\n",
            "Preflight warnings:\n",
            " - 4-bit bitsandbytes is unreliable on native Windows; use WSL or disable load_in_4bit.\n",
            " - GPU reports 4.0 GB total; may OOM with current settings.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "hf_token = train_dpo.resolve_hf_token(config)\n",
        "train_dpo.preflight_checks(config, hf_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2b14ac5",
      "metadata": {},
      "source": [
        "## Dry run (no training)\n",
        "Run this to validate config, dataset path, and GPU before starting a full run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2ad64777",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Starting Llama-3.2-1B DPO Training with QLoRA\n",
            "============================================================\n",
            "\n",
            "Step 1/8: Loading configuration...\n",
            "Configuration loaded from c:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\00_configs\\dpo.json\n",
            "Secrets loaded from C:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\00_configs\\secrets.toml\n",
            "HuggingFace token configured\n",
            "Running preflight checks...\n",
            "\n",
            "Preflight warnings:\n",
            " - 4-bit bitsandbytes is unreliable on native Windows; use WSL or disable load_in_4bit.\n",
            " - GPU reports 4.0 GB total; may OOM with current settings.\n",
            "\n",
            "Dry run complete. Exiting before model/dataset load.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "orig_argv = sys.argv[:]\n",
        "sys.argv = [\"train_dpo.py\", \"--config\", str(CONFIG_PATH), \"--dry_run\"]\n",
        "try:\n",
        "    train_dpo.main()\n",
        "finally:\n",
        "    sys.argv = orig_argv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d6a5531",
      "metadata": {},
      "source": [
        "## Start training\n",
        "This will launch DPO training and write logs to `05_logs/training.log`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2b5e576f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Starting Llama-3.2-1B DPO Training with QLoRA\n",
            "============================================================\n",
            "\n",
            "Step 1/8: Loading configuration...\n",
            "Configuration loaded from c:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\00_configs\\dpo.json\n",
            "Secrets loaded from C:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\00_configs\\secrets.toml\n",
            "HuggingFace token configured\n",
            "Running preflight checks...\n",
            "\n",
            "Preflight warnings:\n",
            " - 4-bit bitsandbytes is unreliable on native Windows; use WSL or disable load_in_4bit.\n",
            " - GPU reports 4.0 GB total; may OOM with current settings.\n",
            "\n",
            "\n",
            "Step 2/8: Setting up 4-bit quantization...\n",
            "BitsAndBytes config created (4-bit quantization enabled)\n",
            "\n",
            "Step 3/8: Loading policy model...\n",
            "Loading base model: meta-llama/Llama-3.2-1B-Instruct\n",
            "This may take a few minutes...\n",
            "Base model loaded with 4-bit quantization (use_cache=False)\n",
            "\n",
            "Step 4/8: Loading tokenizer...\n",
            "Loading tokenizer for: meta-llama/Llama-3.2-1B-Instruct\n",
            "Tokenizer loaded (vocab size: 128256)\n",
            "\n",
            "Step 5/8: Loading and preparing dataset...\n",
            "Loading local dataset from: C:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\01_data\\dpo\\train.jsonl\n",
            "Loaded 120 examples from local file\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Filter: 100%|██████████| 120/120 [00:00<?, ? examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After filtering empty rows: 120 examples\n",
            "Using custom formatting for DPO\n",
            "\n",
            "Step 6/8: Setting up LoRA...\n",
            "LoRA config created\n",
            "Preparing policy model for training...\n",
            "Policy model ready\n",
            "Trainable params: 5,636,096 / 754,911,232 (0.75%)\n",
            "\n",
            "Step 7/8: Loading reference model...\n",
            "Loading base model: meta-llama/Llama-3.2-1B-Instruct\n",
            "This may take a few minutes...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "c:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\venv\\Lib\\site-packages\\trl\\trainer\\dpo_config.py:176: FutureWarning: The `max_target_length` argument is deprecated in favor of `max_completion_length` and will be removed in a future version.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base model loaded with 4-bit quantization (use_cache=True)\n",
            "\n",
            "Step 8/8: Setting up DPO trainer...\n",
            "Training arguments configured\n",
            "Formatting dataset for DPO...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Filter: 100%|██████████| 120/120 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 120/120 [00:00<00:00, 36538.40 examples/s]\n",
            "Trainer.tokenizer is now deprecated. You should use `Trainer.processing_class = processing_class` instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset formatted for DPO with 120 examples\n",
            "Dataset formatted for DPO: 120 examples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing train dataset: 100%|██████████| 120/120 [00:00<00:00, 2597.74 examples/s]\n",
            "c:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\venv\\Lib\\site-packages\\trl\\trainer\\dpo_trainer.py:822: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `DPOTrainer.__init__`. Use `processing_class` instead.\n",
            "  super().__init__(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DPO trainer ready\n",
            "\n",
            "============================================================\n",
            "Starting DPO training...\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/45 [00:00<?, ?it/s]Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
            " 11%|█         | 5/45 [00:29<03:17,  4.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.693, 'grad_norm': 29.93354606628418, 'learning_rate': 1.9974521146102535e-05, 'rewards/chosen': 0.0004898930201306939, 'rewards/rejected': 0.00026531220646575093, 'rewards/accuracies': 0.4749999940395355, 'rewards/margins': 0.00022458071180153638, 'logps/rejected': -68.32734680175781, 'logps/chosen': -105.6583251953125, 'logits/rejected': 0.5617692470550537, 'logits/chosen': 1.4101569652557373, 'epoch': 0.33}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 22%|██▏       | 10/45 [00:51<02:41,  4.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.6709, 'grad_norm': 26.494922637939453, 'learning_rate': 1.9096319953545186e-05, 'rewards/chosen': 0.03728938102722168, 'rewards/rejected': -0.008025208488106728, 'rewards/accuracies': 0.9750000238418579, 'rewards/margins': 0.04531458765268326, 'logps/rejected': -71.74079132080078, 'logps/chosen': -111.71540832519531, 'logits/rejected': 0.4779233932495117, 'logits/chosen': 1.4357082843780518, 'epoch': 0.67}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 33%|███▎      | 15/45 [01:13<02:16,  4.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.6243, 'grad_norm': 27.308359146118164, 'learning_rate': 1.7071067811865477e-05, 'rewards/chosen': 0.0987069383263588, 'rewards/rejected': -0.04551834613084793, 'rewards/accuracies': 1.0, 'rewards/margins': 0.14422526955604553, 'logps/rejected': -74.43171691894531, 'logps/chosen': -114.6983413696289, 'logits/rejected': 0.47051963210105896, 'logits/chosen': 1.361846685409546, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 44%|████▍     | 20/45 [01:41<01:55,  4.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.5323, 'grad_norm': 29.316051483154297, 'learning_rate': 1.4154150130018867e-05, 'rewards/chosen': 0.24226300418376923, 'rewards/rejected': -0.11616505682468414, 'rewards/accuracies': 1.0, 'rewards/margins': 0.35842806100845337, 'logps/rejected': -71.35551452636719, 'logps/chosen': -111.901123046875, 'logits/rejected': 0.45166015625, 'logits/chosen': 1.3239576816558838, 'epoch': 1.33}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 56%|█████▌    | 25/45 [02:04<01:29,  4.47s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4909, 'grad_norm': 24.420854568481445, 'learning_rate': 1.0713391831992324e-05, 'rewards/chosen': 0.3223472237586975, 'rewards/rejected': -0.14761564135551453, 'rewards/accuracies': 1.0, 'rewards/margins': 0.4699628949165344, 'logps/rejected': -70.67399597167969, 'logps/chosen': -107.5689926147461, 'logits/rejected': 0.4898737072944641, 'logits/chosen': 1.2856556177139282, 'epoch': 1.67}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 67%|██████▋   | 30/45 [02:26<01:08,  4.54s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4664, 'grad_norm': 24.03462028503418, 'learning_rate': 7.182674431585703e-06, 'rewards/chosen': 0.346079558134079, 'rewards/rejected': -0.19250282645225525, 'rewards/accuracies': 1.0, 'rewards/margins': 0.538582444190979, 'logps/rejected': -76.50038146972656, 'logps/chosen': -104.85993957519531, 'logits/rejected': 0.5537185072898865, 'logits/chosen': 1.3225390911102295, 'epoch': 2.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 78%|███████▊  | 35/45 [02:56<00:49,  4.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.418, 'grad_norm': 25.153297424316406, 'learning_rate': 4.007223334886531e-06, 'rewards/chosen': 0.41068607568740845, 'rewards/rejected': -0.2698691487312317, 'rewards/accuracies': 1.0, 'rewards/margins': 0.6805551648139954, 'logps/rejected': -74.59923553466797, 'logps/chosen': -110.28421783447266, 'logits/rejected': 0.4506847858428955, 'logits/chosen': 1.2197239398956299, 'epoch': 2.33}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 89%|████████▉ | 40/45 [03:18<00:22,  4.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4023, 'grad_norm': 23.915508270263672, 'learning_rate': 1.587464671688187e-06, 'rewards/chosen': 0.4470794200897217, 'rewards/rejected': -0.2854829430580139, 'rewards/accuracies': 1.0, 'rewards/margins': 0.7325623035430908, 'logps/rejected': -71.83866119384766, 'logps/chosen': -100.33122253417969, 'logits/rejected': 0.5067847967147827, 'logits/chosen': 1.2323968410491943, 'epoch': 2.67}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 45/45 [03:41<00:00,  4.44s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3827, 'grad_norm': 21.932559967041016, 'learning_rate': 2.2853134028840594e-07, 'rewards/chosen': 0.4800655245780945, 'rewards/rejected': -0.30899205803871155, 'rewards/accuracies': 1.0, 'rewards/margins': 0.7890576124191284, 'logps/rejected': -76.17259216308594, 'logps/chosen': -109.44319152832031, 'logits/rejected': 0.5126221776008606, 'logits/chosen': 1.244204044342041, 'epoch': 3.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 45/45 [03:41<00:00,  4.93s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 221.9855, 'train_samples_per_second': 1.622, 'train_steps_per_second': 0.203, 'train_loss': 0.5201077408260769, 'epoch': 3.0}\n",
            "\n",
            "============================================================\n",
            "Saving policy LoRA adapter and tokenizer...\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "DPO training complete!\n",
            "Policy adapter saved to: 04_models\\adapters\\output_dpo\n",
            "============================================================\n",
            "\n",
            "Next steps:\n",
            "1. Run inference with adapter: python 02_src/run_inference.py --adapter_path 04_models/adapters/output_dpo\n",
            "2. Merge LoRA: python 02_src/merge_lora.py --adapter_path 04_models/adapters/output_dpo --output_path 04_models/merged/merged_model_dpo\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "orig_argv = sys.argv[:]\n",
        "sys.argv = [\"train_dpo.py\", \"--config\", str(CONFIG_PATH)]\n",
        "try:\n",
        "    train_dpo.main()\n",
        "finally:\n",
        "    sys.argv = orig_argv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a34a5e1c",
      "metadata": {},
      "source": [
        "### Although it is hard to define the magic numbers for the training loss and epoch, loss of approximately 0.4, and around 3 epochs is often a sign the run is good enough!!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d63b0651",
      "metadata": {},
      "source": [
        "## Merge\n",
        "Merge the trained LoRA adapters into a full, standalone model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d1d6947b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Merging LoRA adapters into base model\n",
            "============================================================\n",
            "\n",
            "Loading base model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\venv\\Lib\\site-packages\\accelerate\\utils\\modeling.py:1390: UserWarning: Current model requires 1088 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base model loaded\n",
            "\n",
            "Loading LoRA adapters...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\venv\\Lib\\site-packages\\accelerate\\utils\\modeling.py:1390: UserWarning: Current model requires 2176 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LoRA adapters loaded\n",
            "\n",
            "Merging adapters...\n",
            "Adapters merged successfully\n",
            "\n",
            "Loading tokenizer...\n",
            "Tokenizer loaded\n",
            "\n",
            "Saving merged model to: c:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\04_models\\merged\\merged_model_dpo\n",
            "Merged model saved\n",
            "\n",
            "Merge complete!\n",
            "Merged model location: c:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\04_models\\merged\\merged_model_dpo\n",
            "Use like any HF model:\n",
            "  model = AutoModelForCausalLM.from_pretrained(\"c:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\04_models\\merged\\merged_model_dpo\")\n",
            "  tokenizer = AutoTokenizer.from_pretrained(\"c:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\04_models\\merged\\merged_model_dpo\")\n"
          ]
        }
      ],
      "source": [
        "# Purpose: merge LoRA adapters into the base model for a standalone checkpoint.\n",
        "from pathlib import Path\n",
        "\n",
        "BASE_MODEL = config.get(\"model_name\", \"meta-llama/Llama-3.2-1B-Instruct\")\n",
        "ADAPTER_PATH = Path(config.get(\"output_dir\", REPO_ROOT / \"04_models\" / \"adapters\" / \"output_dpo\"))\n",
        "if not ADAPTER_PATH.is_absolute():\n",
        "    ADAPTER_PATH = REPO_ROOT / ADAPTER_PATH\n",
        "\n",
        "OUTPUT_PATH = REPO_ROOT / \"04_models\" / \"merged\" / \"merged_model_dpo\"\n",
        "\n",
        "merge_lora.merge_lora_to_base(\n",
        "    base_model_name=BASE_MODEL,\n",
        "    adapter_path=ADAPTER_PATH,\n",
        "    output_path=OUTPUT_PATH,\n",
        "    push_to_hub=False,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cb1bfff",
      "metadata": {},
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "e0b1e6e2",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "d60256eb",
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"Explain 'Dynamic Positioning' (DP).\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eccdd16b",
      "metadata": {},
      "source": [
        "## Base model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "603510f3",
      "metadata": {},
      "source": [
        "#### This is the pre-trained model that we used for the post-training (DPO). The model does not have a given domain knowledge that we provided"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "f6748f2e",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\venv\\Lib\\site-packages\\accelerate\\utils\\modeling.py:1390: UserWarning: Current model requires 1088 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "base_model_path = \"meta-llama/Llama-3.2-1B-Instruct\" \n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_path, token=hf_token)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_path,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    token=hf_token,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebc85f03",
      "metadata": {},
      "source": [
        "#### When we ask about the terminology (e.g., DP), the base model shows an understanding that is not grounded on the given domain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37b12bf0",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dynamic Positioning (DP) is a type of broadcasting technology used to transmit high-definition video content to a wide range of devices, including mobile devices, set-top boxes, and streaming devices. It allows for real-time video transmission and reception, enabling users to watch live or pre-recorded content in a seamless and interactive way.\n",
            "\n",
            "Here's how it works:\n",
            "\n",
            "**Key Components:**\n",
            "\n",
            "1. **Transmitter:** The transmitter is the device that converts the video signal from the broadcast source (e.g., satellite, cable, or terrestrial) into a digital format that can be transmitted wirelessly.\n",
            "2. **Receiver:** The receiver is the device\n"
          ]
        }
      ],
      "source": [
        "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "chat_prompt = tokenizer.apply_chat_template(\n",
        "    messages, tokenize=False, add_generation_prompt=True\n",
        ")\n",
        "\n",
        "inputs = tokenizer(chat_prompt, return_tensors=\"pt\").to(model.device)\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=128,\n",
        "    do_sample=True,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "\n",
        "reply = tokenizer.decode(\n",
        "    outputs[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True\n",
        ").strip()\n",
        "print(reply)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4da8dcbe",
      "metadata": {},
      "source": [
        "## Load the post-trained model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b1e572e",
      "metadata": {},
      "source": [
        "#### Now we call the post-trained model with Direct Preference Optimization (DPO)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53c0050d",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\venv\\Lib\\site-packages\\accelerate\\utils\\modeling.py:1390: UserWarning: Current model requires 1088 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model_path = r\"04_models\\merged\\merged_model_dpo\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "601c13d8",
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()\n",
        "model.config.use_cache = True  # faster generation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8d634fe",
      "metadata": {},
      "source": [
        "### Generate the output from the post-traned model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57fcc60f",
      "metadata": {},
      "source": [
        "#### The post-trained model shows a better understanding in the domain knowledge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "872b6996",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'\n",
            "* Explain the concept of 'Dynamic Positioning' (DP) in the context of offshore oil and gas operations.\n",
            "* Discuss the benefits and challenges of using DP in offshore oil and gas operations.\n",
            "* Describe the different types of DP systems used in offshore oil and gas operations.\n",
            "* Explain the role of DP in the development of offshore oil and gas platforms.\n",
            "* Discuss the importance of DP in the safety and efficiency of offshore oil and gas operations.\n",
            "* Explain the role of DP in the environmental impact of offshore oil and gas operations.\n",
            "* Discuss the future of DP in offshore oil and gas operations.\n",
            "\n",
            "## Step 1: Introduction to Dynamic Positioning (DP)\n",
            "Dynamic Positioning (DP) is a technique used to maintain a stable position of an offshore platform or vessel in the ocean. It involves using a combination of thrusters and stabilizers to keep the platform or vessel at a fixed depth and position.\n",
            "\n",
            "## Step 2: Benefits of Dynamic Positioning (DP)\n",
            "The benefits of DP include:\n",
            "* Improved safety: DP reduces the risk of accidents caused by platform or vessel instability.\n",
            "* Increased efficiency: DP allows for more efficient use of resources, such as fuel and personnel.\n",
            "* Enhanced productivity: DP enables the platform or vessel to operate at higher productivity levels.\n",
            "\n",
            "## Step\n"
          ]
        }
      ],
      "source": [
        "inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(model.device)\n",
        "\n",
        "gen_kwargs = {\n",
        "    \"max_new_tokens\": 2**8,\n",
        "    \"do_sample\": False,  # set True only if you need sampling\n",
        "    \"eos_token_id\": tokenizer.eos_token_id,\n",
        "    \"pad_token_id\": tokenizer.pad_token_id,\n",
        "}\n",
        "\n",
        "with torch.inference_mode():\n",
        "    outputs = model.generate(**inputs, **gen_kwargs)\n",
        "\n",
        "# Decode only newly generated tokens\n",
        "new_tokens = outputs[0][inputs[\"input_ids\"].shape[1]:]\n",
        "response = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c4ea637",
      "metadata": {},
      "source": [
        "# Explanations"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
