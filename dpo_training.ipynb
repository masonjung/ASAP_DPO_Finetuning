{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Llama-3.2 DPO Training (Jupyter)\n",
        "\n",
        "This notebook mirrors `02_src/train_dpo.py` and uses the repo layout.\n",
        "- Launch Jupyter from the repo root so paths resolve.\n",
        "- Set your Hugging Face token in `00_configs/secrets.toml` or `HF_TOKEN`.\n",
        "- If 4-bit bitsandbytes fails on Windows, set `load_in_4bit` to false in `00_configs/dpo.json` or use WSL.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overall process\n",
        "1. Resolve repo paths and import code from `02_src`.\n",
        "2. Check GPU + CUDA availability.\n",
        "3. Load config from `00_configs/dpo.json` and validate paths.\n",
        "4. Run preflight checks (token, dataset, VRAM).\n",
        "5. Optional dry run (no training).\n",
        "6. Train DPO and save adapters + tokenizer.\n",
        "7. Compare base vs tuned responses (fixed prompts).\n",
        "8. Optional: merge LoRA adapters into a standalone model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "710a852b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Repo root: c:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\n",
            "Source dir: c:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\02_src\n"
          ]
        }
      ],
      "source": [
        "# Purpose: resolve repo root and add 02_src to sys.path for imports.\n",
        "# This keeps notebook paths consistent with the repo layout.\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "REPO_ROOT = Path.cwd()\n",
        "if not (REPO_ROOT / \"00_configs\").exists():\n",
        "    for parent in REPO_ROOT.parents:\n",
        "        if (parent / \"00_configs\").exists():\n",
        "            REPO_ROOT = parent\n",
        "            break\n",
        "\n",
        "if not (REPO_ROOT / \"00_configs\").exists():\n",
        "    raise RuntimeError(\"Could not find repo root containing 00_configs.\")\n",
        "\n",
        "SRC_DIR = REPO_ROOT / \"02_src\"\n",
        "if str(SRC_DIR) not in sys.path:\n",
        "    sys.path.insert(0, str(SRC_DIR))\n",
        "\n",
        "print(\"Repo root:\", REPO_ROOT)\n",
        "print(\"Source dir:\", SRC_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9ff506fc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda_available: True\n",
            "device_name: NVIDIA RTX A1000 Laptop GPU\n",
            "mem_free/total_GB: 3.46/4.29\n"
          ]
        }
      ],
      "source": [
        "# Purpose: quick GPU/CUDA sanity check (availability and memory).\n",
        "import torch\n",
        "print(\"cuda_available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"device_name:\", torch.cuda.get_device_name(0))\n",
        "    free, total = torch.cuda.mem_get_info()\n",
        "    print(f\"mem_free/total_GB: {free/1e9:.2f}/{total/1e9:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "16107dc9",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Imports OK\n"
          ]
        }
      ],
      "source": [
        "# Purpose: import project modules used across later cells.\n",
        "import importlib\n",
        "\n",
        "train_dpo = importlib.import_module(\"train_dpo\")\n",
        "run_inference = importlib.import_module(\"run_inference\")\n",
        "merge_lora = importlib.import_module(\"merge_lora\")\n",
        "eval_module = importlib.import_module(\"eval.evaluate\")\n",
        "data_utils = importlib.import_module(\"utils.data_utils\")\n",
        "formatting = importlib.import_module(\"utils.formatting\")\n",
        "\n",
        "print(\"Imports OK\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3fac907f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration loaded from c:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\00_configs\\dpo.json\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'model_name': 'meta-llama/Llama-3.2-1B-Instruct',\n",
              " 'dataset_hf': 'c:\\\\Users\\\\Minseok Jung\\\\Desktop\\\\Programming\\\\asap_finetuning\\\\01_data\\\\dpo\\\\train.jsonl',\n",
              " 'output_dir': 'c:\\\\Users\\\\Minseok Jung\\\\Desktop\\\\Programming\\\\asap_finetuning\\\\04_models\\\\adapters\\\\output_dpo',\n",
              " 'max_seq_length': 512,\n",
              " 'max_prompt_length': 256,\n",
              " 'max_target_length': 256,\n",
              " 'num_train_epochs': 1,\n",
              " 'per_device_train_batch_size': 1,\n",
              " 'gradient_accumulation_steps': 4,\n",
              " 'learning_rate': 5e-05,\n",
              " 'lr_scheduler_type': 'cosine',\n",
              " 'warmup_ratio': 0.03,\n",
              " 'weight_decay': 0.01,\n",
              " 'dataloader_num_workers': 2,\n",
              " 'logging_steps': 10,\n",
              " 'save_steps': 200,\n",
              " 'save_total_limit': 2,\n",
              " 'fp16': True,\n",
              " 'bf16': False,\n",
              " 'optim': 'paged_adamw_8bit',\n",
              " 'gradient_checkpointing': True,\n",
              " 'lora_r': 16,\n",
              " 'lora_alpha': 32,\n",
              " 'lora_dropout': 0.05,\n",
              " 'lora_target_modules': ['q_proj',\n",
              "  'k_proj',\n",
              "  'v_proj',\n",
              "  'o_proj',\n",
              "  'gate_proj',\n",
              "  'up_proj',\n",
              "  'down_proj'],\n",
              " 'load_in_4bit': True,\n",
              " 'bnb_4bit_use_double_quant': True,\n",
              " 'bnb_4bit_quant_type': 'nf4',\n",
              " 'bnb_4bit_compute_dtype': 'float16',\n",
              " 'dpo_beta': 0.1,\n",
              " 'seed': 42,\n",
              " 'dataset_split': 'train',\n",
              " 'max_train_samples': None,\n",
              " 'dataset_shuffle': True,\n",
              " 'num_proc': None}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Purpose: load the DPO training config from 00_configs/dpo.json.\n",
        "CONFIG_PATH = REPO_ROOT / \"00_configs\" / \"dpo.json\"\n",
        "config = train_dpo.load_config(CONFIG_PATH)\n",
        "config\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a298be98",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Secrets loaded from C:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\00_configs\\secrets.toml\n",
            "HuggingFace token configured\n",
            "\n",
            "Preflight warnings:\n",
            " - 4-bit bitsandbytes is unreliable on native Windows; use WSL or disable load_in_4bit.\n",
            " - GPU reports 4.0 GB total; may OOM with current settings.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Purpose: resolve HF token and run preflight validation checks.\n",
        "hf_token = train_dpo.resolve_hf_token(config)\n",
        "train_dpo.preflight_checks(config, hf_token)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2b14ac5",
      "metadata": {},
      "source": [
        "## Optional: dry run (no training)\n",
        "Run this to validate config, dataset path, and GPU before starting a long run.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2ad64777",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Starting Llama-3.2-1B DPO Training with QLoRA\n",
            "============================================================\n",
            "\n",
            "Step 1/8: Loading configuration...\n",
            "Configuration loaded from c:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\00_configs\\dpo.json\n",
            "Secrets loaded from C:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\00_configs\\secrets.toml\n",
            "HuggingFace token configured\n",
            "Running preflight checks...\n",
            "\n",
            "Preflight warnings:\n",
            " - 4-bit bitsandbytes is unreliable on native Windows; use WSL or disable load_in_4bit.\n",
            " - GPU reports 4.0 GB total; may OOM with current settings.\n",
            "\n",
            "Dry run complete. Exiting before model/dataset load.\n"
          ]
        }
      ],
      "source": [
        "# Purpose: run a dry-run to validate config and dataset without training.\n",
        "import sys\n",
        "\n",
        "orig_argv = sys.argv[:]\n",
        "sys.argv = [\"train_dpo.py\", \"--config\", str(CONFIG_PATH), \"--dry_run\"]\n",
        "try:\n",
        "    train_dpo.main()\n",
        "finally:\n",
        "    sys.argv = orig_argv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d6a5531",
      "metadata": {},
      "source": [
        "## Start training\n",
        "This will launch DPO training and write logs to `05_logs/training.log`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "2b5e576f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Starting Llama-3.2-1B DPO Training with QLoRA\n",
            "============================================================\n",
            "\n",
            "Step 1/8: Loading configuration...\n",
            "Configuration loaded from c:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\00_configs\\dpo.json\n",
            "Secrets loaded from C:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\00_configs\\secrets.toml\n",
            "HuggingFace token configured\n",
            "Running preflight checks...\n",
            "\n",
            "Preflight warnings:\n",
            " - 4-bit bitsandbytes is unreliable on native Windows; use WSL or disable load_in_4bit.\n",
            " - GPU reports 4.0 GB total; may OOM with current settings.\n",
            "\n",
            "\n",
            "Step 2/8: Setting up 4-bit quantization...\n",
            "BitsAndBytes config created (4-bit quantization enabled)\n",
            "\n",
            "Step 3/8: Loading policy model...\n",
            "Loading base model: meta-llama/Llama-3.2-1B-Instruct\n",
            "This may take a few minutes...\n",
            "Base model loaded with 4-bit quantization (use_cache=False)\n",
            "\n",
            "Step 4/8: Loading tokenizer...\n",
            "Loading tokenizer for: meta-llama/Llama-3.2-1B-Instruct\n",
            "Tokenizer loaded (vocab size: 128256)\n",
            "\n",
            "Step 5/8: Loading and preparing dataset...\n",
            "Loading local dataset from: c:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\01_data\\dpo\\train.jsonl\n",
            "Loaded 2 examples from local file\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Filter: 100%|██████████| 2/2 [00:00<00:00, 804.12 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After filtering empty rows: 2 examples\n",
            "Using custom formatting for DPO\n",
            "\n",
            "Step 6/8: Setting up LoRA...\n",
            "LoRA config created\n",
            "Preparing policy model for training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Policy model ready\n",
            "Trainable params: 11,272,192 / 760,547,328 (1.48%)\n",
            "\n",
            "Step 7/8: Loading reference model...\n",
            "Loading base model: meta-llama/Llama-3.2-1B-Instruct\n",
            "This may take a few minutes...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\venv\\Lib\\site-packages\\trl\\trainer\\dpo_config.py:176: FutureWarning: The `max_target_length` argument is deprecated in favor of `max_completion_length` and will be removed in a future version.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base model loaded with 4-bit quantization (use_cache=True)\n",
            "\n",
            "Step 8/8: Setting up DPO trainer...\n",
            "Training arguments configured\n",
            "Formatting dataset for DPO...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Filter: 100%|██████████| 2/2 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 2/2 [00:00<00:00, 1757.88 examples/s]\n",
            "Trainer.tokenizer is now deprecated. You should use `Trainer.processing_class = processing_class` instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset formatted for DPO with 2 examples\n",
            "Dataset formatted for DPO: 2 examples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing train dataset: 100%|██████████| 2/2 [00:00<00:00, 186.42 examples/s]\n",
            "c:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\venv\\Lib\\site-packages\\trl\\trainer\\dpo_trainer.py:822: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `DPOTrainer.__init__`. Use `processing_class` instead.\n",
            "  super().__init__(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DPO trainer ready\n",
            "\n",
            "============================================================\n",
            "Starting DPO training...\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1 [00:00<?, ?it/s]Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
            "100%|██████████| 1/1 [00:07<00:00,  7.91s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 7.9145, 'train_samples_per_second': 0.253, 'train_steps_per_second': 0.126, 'train_loss': 0.3459859788417816, 'epoch': 1.0}\n",
            "\n",
            "============================================================\n",
            "Saving policy LoRA adapter and tokenizer...\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "DPO training complete!\n",
            "Policy adapter saved to: c:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\04_models\\adapters\\output_dpo\n",
            "============================================================\n",
            "\n",
            "Next steps:\n",
            "1. Run inference with adapter: python 02_src/run_inference.py --adapter_path 04_models/adapters/output_dpo\n",
            "2. Merge LoRA: python 02_src/merge_lora.py --adapter_path 04_models/adapters/output_dpo --output_path 04_models/merged/merged_model_dpo\n"
          ]
        }
      ],
      "source": [
        "# Purpose: launch full DPO training using the CLI entrypoint.\n",
        "import sys\n",
        "\n",
        "orig_argv = sys.argv[:]\n",
        "sys.argv = [\"train_dpo.py\", \"--config\", str(CONFIG_PATH)]\n",
        "try:\n",
        "    train_dpo.main()\n",
        "finally:\n",
        "    sys.argv = orig_argv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d63b0651",
      "metadata": {},
      "source": [
        "## Merge\n",
        "Merge the trained LoRA adapters into a full, standalone model.\n",
        "This is optional; skip if you only need adapters for inference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "d1d6947b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Merging LoRA adapters into base model\n",
            "============================================================\n",
            "\n",
            "Loading base model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\venv\\Lib\\site-packages\\accelerate\\utils\\modeling.py:1390: UserWarning: Current model requires 1088 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base model loaded\n",
            "\n",
            "Loading LoRA adapters...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\venv\\Lib\\site-packages\\accelerate\\utils\\modeling.py:1390: UserWarning: Current model requires 2176 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LoRA adapters loaded\n",
            "\n",
            "Merging adapters...\n",
            "Adapters merged successfully\n",
            "\n",
            "Loading tokenizer...\n",
            "Tokenizer loaded\n",
            "\n",
            "Saving merged model to: c:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\04_models\\merged\\merged_model_dpo\n",
            "Merged model saved\n",
            "\n",
            "Merge complete!\n",
            "Merged model location: c:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\04_models\\merged\\merged_model_dpo\n",
            "Use like any HF model:\n",
            "  model = AutoModelForCausalLM.from_pretrained(\"c:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\04_models\\merged\\merged_model_dpo\")\n",
            "  tokenizer = AutoTokenizer.from_pretrained(\"c:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\04_models\\merged\\merged_model_dpo\")\n"
          ]
        }
      ],
      "source": [
        "# Purpose: merge LoRA adapters into the base model for a standalone checkpoint.\n",
        "from pathlib import Path\n",
        "\n",
        "BASE_MODEL = config.get(\"model_name\", \"meta-llama/Llama-3.2-1B-Instruct\")\n",
        "ADAPTER_PATH = Path(config.get(\"output_dir\", REPO_ROOT / \"04_models\" / \"adapters\" / \"output_dpo\"))\n",
        "if not ADAPTER_PATH.is_absolute():\n",
        "    ADAPTER_PATH = REPO_ROOT / ADAPTER_PATH\n",
        "\n",
        "OUTPUT_PATH = REPO_ROOT / \"04_models\" / \"merged\" / \"merged_model_dpo\"\n",
        "\n",
        "merge_lora.merge_lora_to_base(\n",
        "    base_model_name=BASE_MODEL,\n",
        "    adapter_path=ADAPTER_PATH,\n",
        "    output_path=OUTPUT_PATH,\n",
        "    push_to_hub=False,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cb1bfff",
      "metadata": {},
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "53c0050d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model_path = r\"04_models\\merged\\merged_model_dpo\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "6950dc4f",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### Instruction:\n",
            "What is upstream in the oil and gas industry?\n",
            "\n",
            "### Response:\n",
            "In the oil and gas industry, \"upstream\" refers to the exploration and production of crude oil and natural gas. This includes the activities involved in finding, extracting, and processing these resources.\n",
            "\n",
            "### Example:\n",
            "- A company like ExxonMobil is an example of an oil and gas company that operates upstream.\n",
            "- A company like Chevron is also an example of an oil and gas company that operates upstream.\n",
            "\n",
            "### Key points to note:\n",
            "- Upstream activities typically involve drilling, exploration, and production of crude oil and natural gas.\n",
            "- These activities can be performed in various locations, such as onshore or offshore, depending on the geology\n"
          ]
        }
      ],
      "source": [
        "prompt = \"### Instruction:\\nWhat is upstream in the oil and gas industry?\\n\\n### Response:\\n\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "outputs = model.generate(**inputs, max_new_tokens=128, do_sample=True, temperature=0.7, top_p=0.9)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c4ea637",
      "metadata": {},
      "source": [
        "# Explanations"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
