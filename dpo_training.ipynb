{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# On-device Direct Preference Optimization (DPO) \n",
        "\n",
        "This notebook can be used instead of the `02_src/train_dpo.py`.\n",
        "- Launch Jupyter from the repo root so paths resolve.\n",
        "- Set your Hugging Face token in `00_configs/secrets.toml` or `HF_TOKEN`.\n",
        "- Get a model access approvel that you would like to use.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70f3bba3",
      "metadata": {},
      "source": [
        "## Check the repository root"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c071f7a7",
      "metadata": {},
      "source": [
        "### You should assign the repository root for your device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "710a852b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Repo root: c:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\n",
            "Source dir: c:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\02_src\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "REPO_ROOT = Path.cwd()\n",
        "if not (REPO_ROOT / \"00_configs\").exists():\n",
        "    for parent in REPO_ROOT.parents:\n",
        "        if (parent / \"00_configs\").exists():\n",
        "            REPO_ROOT = parent\n",
        "            break\n",
        "\n",
        "if not (REPO_ROOT / \"00_configs\").exists():\n",
        "    raise RuntimeError(\"Could not find repo root containing 00_configs.\")\n",
        "\n",
        "SRC_DIR = REPO_ROOT / \"02_src\"\n",
        "if str(SRC_DIR) not in sys.path:\n",
        "    sys.path.insert(0, str(SRC_DIR))\n",
        "\n",
        "print(\"Repo root:\", REPO_ROOT)\n",
        "print(\"Source dir:\", SRC_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9f056ab",
      "metadata": {},
      "source": [
        "## Check the hardware, CUDA, and project modules"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa36a2c8",
      "metadata": {},
      "source": [
        "### Check the hardware condition and whether CUDA is available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9ff506fc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda_available: True\n",
            "device_name: NVIDIA RTX A1000 Laptop GPU\n",
            "mem_free/total_GB: 3.46/4.29\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(\"cuda_available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"device_name:\", torch.cuda.get_device_name(0))\n",
        "    free, total = torch.cuda.mem_get_info()\n",
        "    print(f\"mem_free/total_GB: {free/1e9:.2f}/{total/1e9:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de8557b8",
      "metadata": {},
      "source": [
        "### Project modules from the project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "16107dc9",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Imports OK\n"
          ]
        }
      ],
      "source": [
        "import importlib\n",
        "\n",
        "train_dpo = importlib.import_module(\"train_dpo\")\n",
        "run_inference = importlib.import_module(\"run_inference\")\n",
        "merge_lora = importlib.import_module(\"merge_lora\")\n",
        "data_utils = importlib.import_module(\"utils.data_utils\")\n",
        "formatting = importlib.import_module(\"utils.formatting\")\n",
        "\n",
        "# eval_module = importlib.import_module(\"eval.evaluate\")\n",
        "print(\"Imports OK\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ae7ca1f",
      "metadata": {},
      "source": [
        "### Confirm the configuration. You can adjust the paths and hyperparameters in 00_configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "3fac907f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration loaded from c:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\00_configs\\dpo.json\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'model_name': 'meta-llama/Llama-3.2-1B-Instruct',\n",
              " 'dataset_hf': '01_data\\\\dpo\\\\train.jsonl',\n",
              " 'output_dir': '04_models\\\\adapters\\\\output_dpo',\n",
              " 'max_seq_length': 512,\n",
              " 'max_prompt_length': 256,\n",
              " 'max_target_length': 256,\n",
              " 'num_train_epochs': 1,\n",
              " 'per_device_train_batch_size': 1,\n",
              " 'gradient_accumulation_steps': 4,\n",
              " 'learning_rate': 5e-05,\n",
              " 'lr_scheduler_type': 'cosine',\n",
              " 'warmup_ratio': 0.03,\n",
              " 'weight_decay': 0.01,\n",
              " 'dataloader_num_workers': 2,\n",
              " 'logging_steps': 10,\n",
              " 'save_steps': 200,\n",
              " 'save_total_limit': 2,\n",
              " 'fp16': True,\n",
              " 'bf16': False,\n",
              " 'optim': 'paged_adamw_8bit',\n",
              " 'gradient_checkpointing': True,\n",
              " 'lora_r': 16,\n",
              " 'lora_alpha': 32,\n",
              " 'lora_dropout': 0.05,\n",
              " 'lora_target_modules': ['q_proj',\n",
              "  'k_proj',\n",
              "  'v_proj',\n",
              "  'o_proj',\n",
              "  'gate_proj',\n",
              "  'up_proj',\n",
              "  'down_proj'],\n",
              " 'load_in_4bit': True,\n",
              " 'bnb_4bit_use_double_quant': True,\n",
              " 'bnb_4bit_quant_type': 'nf4',\n",
              " 'bnb_4bit_compute_dtype': 'float16',\n",
              " 'dpo_beta': 0.1,\n",
              " 'seed': 42,\n",
              " 'dataset_split': 'train',\n",
              " 'max_train_samples': None,\n",
              " 'dataset_shuffle': True,\n",
              " 'num_proc': None}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Purpose: load the DPO training config from 00_configs/dpo.json.\n",
        "CONFIG_PATH = REPO_ROOT / \"00_configs\" / \"dpo.json\"\n",
        "config = train_dpo.load_config(CONFIG_PATH)\n",
        "config"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ae06b90",
      "metadata": {},
      "source": [
        "### HF token check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a298be98",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Secrets loaded from C:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\00_configs\\secrets.toml\n",
            "HuggingFace token configured\n",
            "\n",
            "Preflight warnings:\n",
            " - 4-bit bitsandbytes is unreliable on native Windows; use WSL or disable load_in_4bit.\n",
            " - GPU reports 4.0 GB total; may OOM with current settings.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "hf_token = train_dpo.resolve_hf_token(config)\n",
        "train_dpo.preflight_checks(config, hf_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2b14ac5",
      "metadata": {},
      "source": [
        "## Dry run (no training)\n",
        "Run this to validate config, dataset path, and GPU before starting a full run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "2ad64777",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Starting Llama-3.2-1B DPO Training with QLoRA\n",
            "============================================================\n",
            "\n",
            "Step 1/8: Loading configuration...\n",
            "Configuration loaded from c:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\00_configs\\dpo.json\n",
            "Secrets loaded from C:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\00_configs\\secrets.toml\n",
            "HuggingFace token configured\n",
            "Running preflight checks...\n",
            "\n",
            "Preflight warnings:\n",
            " - 4-bit bitsandbytes is unreliable on native Windows; use WSL or disable load_in_4bit.\n",
            " - GPU reports 4.0 GB total; may OOM with current settings.\n",
            "\n",
            "Dry run complete. Exiting before model/dataset load.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "orig_argv = sys.argv[:]\n",
        "sys.argv = [\"train_dpo.py\", \"--config\", str(CONFIG_PATH), \"--dry_run\"]\n",
        "try:\n",
        "    train_dpo.main()\n",
        "finally:\n",
        "    sys.argv = orig_argv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d6a5531",
      "metadata": {},
      "source": [
        "## Start training\n",
        "This will launch DPO training and write logs to `05_logs/training.log`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "2b5e576f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Starting Llama-3.2-1B DPO Training with QLoRA\n",
            "============================================================\n",
            "\n",
            "Step 1/8: Loading configuration...\n",
            "Configuration loaded from c:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\00_configs\\dpo.json\n",
            "Secrets loaded from C:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\00_configs\\secrets.toml\n",
            "HuggingFace token configured\n",
            "Running preflight checks...\n",
            "\n",
            "Preflight warnings:\n",
            " - 4-bit bitsandbytes is unreliable on native Windows; use WSL or disable load_in_4bit.\n",
            " - GPU reports 4.0 GB total; may OOM with current settings.\n",
            "\n",
            "\n",
            "Step 2/8: Setting up 4-bit quantization...\n",
            "BitsAndBytes config created (4-bit quantization enabled)\n",
            "\n",
            "Step 3/8: Loading policy model...\n",
            "Loading base model: meta-llama/Llama-3.2-1B-Instruct\n",
            "This may take a few minutes...\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m sys.argv = [\u001b[33m\"\u001b[39m\u001b[33mtrain_dpo.py\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m--config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(CONFIG_PATH)]\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[43mtrain_dpo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m      8\u001b[39m     sys.argv = orig_argv\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\02_src\\train_dpo.py:337\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    334\u001b[39m bnb_config = setup_bnb_config(config)\n\u001b[32m    336\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStep 3/8: Loading policy model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m policy_model = \u001b[43mload_base_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhf_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# needed for gradient checkpointing\u001b[39;49;00m\n\u001b[32m    342\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStep 4/8: Loading tokenizer...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    345\u001b[39m tokenizer = get_tokenizer(\n\u001b[32m    346\u001b[39m     config[\u001b[33m\"\u001b[39m\u001b[33mmodel_name\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    347\u001b[39m     config[\u001b[33m\"\u001b[39m\u001b[33mmax_seq_length\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    348\u001b[39m     token=hf_token,\n\u001b[32m    349\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\02_src\\train_dpo.py:145\u001b[39m, in \u001b[36mload_base_model\u001b[39m\u001b[34m(model_name, bnb_config, token, use_cache)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading base model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    143\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mThis may take a few minutes...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbnb_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbnb_4bit_compute_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBase model loaded with 4-bit quantization (use_cache=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muse_cache\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:564\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    562\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._model_mapping.keys():\n\u001b[32m    563\u001b[39m     model_class = _get_model_class(config, \u001b[38;5;28mcls\u001b[39m._model_mapping)\n\u001b[32m--> \u001b[39m\u001b[32m564\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    568\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    569\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    570\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:4174\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4171\u001b[39m     device_map = infer_auto_device_map(model, dtype=target_dtype, **device_map_kwargs)\n\u001b[32m   4173\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4174\u001b[39m         \u001b[43mhf_quantizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4176\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4177\u001b[39m     model.tie_weights()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\venv\\Lib\\site-packages\\transformers\\quantizers\\quantizer_bnb_4bit.py:102\u001b[39m, in \u001b[36mBnb4BitHfQuantizer.validate_environment\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    100\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head.values() \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head.values():\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    103\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mSome modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    104\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mquantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    105\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33min 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    106\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`from_pretrained`. Check \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    107\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mhttps://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    108\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfor more details. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    109\u001b[39m         )\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m version.parse(importlib.metadata.version(\u001b[33m\"\u001b[39m\u001b[33mbitsandbytes\u001b[39m\u001b[33m\"\u001b[39m)) < version.parse(\u001b[33m\"\u001b[39m\u001b[33m0.39.0\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    113\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou have a version of `bitsandbytes` that is not compatible with 4bit inference and training\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    114\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m make sure you have the latest version of `bitsandbytes` installed\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    115\u001b[39m     )\n",
            "\u001b[31mValueError\u001b[39m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "orig_argv = sys.argv[:]\n",
        "sys.argv = [\"train_dpo.py\", \"--config\", str(CONFIG_PATH)]\n",
        "try:\n",
        "    train_dpo.main()\n",
        "finally:\n",
        "    sys.argv = orig_argv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d63b0651",
      "metadata": {},
      "source": [
        "## Merge\n",
        "Merge the trained LoRA adapters into a full, standalone model.\n",
        "This is optional; skip if you only need adapters for inference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "d1d6947b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Merging LoRA adapters into base model\n",
            "============================================================\n",
            "\n",
            "Loading base model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\venv\\Lib\\site-packages\\accelerate\\utils\\modeling.py:1390: UserWarning: Current model requires 1088 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base model loaded\n",
            "\n",
            "Loading LoRA adapters...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\venv\\Lib\\site-packages\\accelerate\\utils\\modeling.py:1390: UserWarning: Current model requires 2176 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LoRA adapters loaded\n",
            "\n",
            "Merging adapters...\n",
            "Adapters merged successfully\n",
            "\n",
            "Loading tokenizer...\n",
            "Tokenizer loaded\n",
            "\n",
            "Saving merged model to: c:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\04_models\\merged\\merged_model_dpo\n",
            "Merged model saved\n",
            "\n",
            "Merge complete!\n",
            "Merged model location: c:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\04_models\\merged\\merged_model_dpo\n",
            "Use like any HF model:\n",
            "  model = AutoModelForCausalLM.from_pretrained(\"c:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\04_models\\merged\\merged_model_dpo\")\n",
            "  tokenizer = AutoTokenizer.from_pretrained(\"c:\\Users\\Minseok Jung\\Desktop\\Programming\\asap_finetuning\\04_models\\merged\\merged_model_dpo\")\n"
          ]
        }
      ],
      "source": [
        "# Purpose: merge LoRA adapters into the base model for a standalone checkpoint.\n",
        "from pathlib import Path\n",
        "\n",
        "BASE_MODEL = config.get(\"model_name\", \"meta-llama/Llama-3.2-1B-Instruct\")\n",
        "ADAPTER_PATH = Path(config.get(\"output_dir\", REPO_ROOT / \"04_models\" / \"adapters\" / \"output_dpo\"))\n",
        "if not ADAPTER_PATH.is_absolute():\n",
        "    ADAPTER_PATH = REPO_ROOT / ADAPTER_PATH\n",
        "\n",
        "OUTPUT_PATH = REPO_ROOT / \"04_models\" / \"merged\" / \"merged_model_dpo\"\n",
        "\n",
        "merge_lora.merge_lora_to_base(\n",
        "    base_model_name=BASE_MODEL,\n",
        "    adapter_path=ADAPTER_PATH,\n",
        "    output_path=OUTPUT_PATH,\n",
        "    push_to_hub=False,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cb1bfff",
      "metadata": {},
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "53c0050d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model_path = r\"04_models\\merged\\merged_model_dpo\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "6950dc4f",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### Instruction:\n",
            "What is upstream in the oil and gas industry?\n",
            "\n",
            "### Response:\n",
            "In the oil and gas industry, \"upstream\" refers to the exploration and production of crude oil and natural gas. This includes the activities involved in finding, extracting, and processing these resources.\n",
            "\n",
            "### Example:\n",
            "- A company like ExxonMobil is an example of an oil and gas company that operates upstream.\n",
            "- A company like Chevron is also an example of an oil and gas company that operates upstream.\n",
            "\n",
            "### Key points to note:\n",
            "- Upstream activities typically involve drilling, exploration, and production of crude oil and natural gas.\n",
            "- These activities can be performed in various locations, such as onshore or offshore, depending on the geology\n"
          ]
        }
      ],
      "source": [
        "prompt = \"### Instruction:\\nWhat is upstream in the oil and gas industry?\\n\\n### Response:\\n\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "outputs = model.generate(**inputs, max_new_tokens=128, do_sample=True, temperature=0.7, top_p=0.9)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c4ea637",
      "metadata": {},
      "source": [
        "# Explanations"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
